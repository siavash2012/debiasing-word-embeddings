{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "!pip install -U scipy~=1.13.0\n",
        "!pip install -U Cython\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U6iHQfvYKtyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ccf497-9609-4590-91f4-6b99d3c8e316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n",
            "Requirement already satisfied: scipy~=1.13.0 in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy~=1.13.0) (1.26.4)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec_path='/content/drive/My Drive/Datasets/GoogleNews-vectors-negative300.bin'\n",
        "word2vec_path='/content/drive/My Drive/Datasets/GoogleNews-vectors-negative300-SLIM.bin.gz'\n",
        "analogies_path='/content/drive/My Drive/Datasets/questions-words.txt'"
      ],
      "metadata": {
        "id": "hl8XEawIWWXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the Word2Vec model directly from the Google Drive file path\n",
        "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "# Example: Check similarity between two words\n",
        "similarity = word2vec_model.similarity('man', 'woman')\n",
        "print(f\"Similarity between 'man' and 'woman': {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3xjJwzaWZW9",
        "outputId": "fd1c4b31-f607-4d02-f93e-9bbf7494ac60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Similarity between 'man' and 'woman': 0.7664012312889099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file, skipping lines starting with ':'\n",
        "df = pd.read_csv(analogies_path, delimiter=\" \", header=None, comment=\":\")\n",
        "\n",
        "# Assign column names\n",
        "df.columns = [\"word_a\", \"word_b\", \"word_c\", \"word_d\"]\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e18nGcDsWekb",
        "outputId": "1d85fad9-8a28-415d-a154-affd397b5ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   word_a  word_b   word_c       word_d\n",
            "0  Athens  Greece  Baghdad         Iraq\n",
            "1  Athens  Greece  Bangkok     Thailand\n",
            "2  Athens  Greece  Beijing        China\n",
            "3  Athens  Greece   Berlin      Germany\n",
            "4  Athens  Greece     Bern  Switzerland\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def find_gender_direction():\n",
        "    \"\"\"Finds and returns a 'gender direction'.\"\"\"\n",
        "    gender_biased_word_pairs = [\n",
        "        (\"she\", \"he\"),\n",
        "        (\"her\", \"his\"),\n",
        "        (\"woman\", \"man\"),\n",
        "        (\"Mary\", \"John\"),\n",
        "        (\"herself\", \"himself\"),\n",
        "        (\"daughter\", \"son\"),\n",
        "        (\"mother\", \"father\"),\n",
        "        (\"gal\", \"guy\"),\n",
        "        (\"girl\", \"boy\"),\n",
        "        (\"vagina\", \"penis\"),\n",
        "        (\"feminine\", \"masculine\")\n",
        "    ]\n",
        "\n",
        "    # Filter out missing words\n",
        "    valid_pairs = [(pair[0], pair[1]) for pair in gender_biased_word_pairs if pair[0] in word2vec_model and pair[1] in word2vec_model]\n",
        "\n",
        "    logging.info(f\"Valid pairs used: {len(valid_pairs)} / {len(gender_biased_word_pairs)}\")\n",
        "\n",
        "    # Compute bias vectors\n",
        "    biases = [word2vec_model[pair[0]] - word2vec_model[pair[1]] for pair in valid_pairs]\n",
        "    reversed_biases = [word2vec_model[pair[1]] - word2vec_model[pair[0]] for pair in valid_pairs]\n",
        "\n",
        "    # Normalize bias vectors before PCA\n",
        "    biases = [vec / np.linalg.norm(vec) for vec in biases]\n",
        "    reversed_biases = [vec / np.linalg.norm(vec) for vec in reversed_biases]\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=1)\n",
        "    pca.fit(np.array(biases + reversed_biases))\n",
        "\n",
        "    # Normalize the gender direction\n",
        "    gender_direction = pca.components_[0]\n",
        "    gender_direction = gender_direction / np.linalg.norm(gender_direction)\n",
        "\n",
        "    logging.info(f\"Gender direction: {gender_direction}\")\n",
        "\n",
        "    return gender_direction\n"
      ],
      "metadata": {
        "id": "cnCouXa4Wo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_direction=find_gender_direction()\n",
        "print(gender_direction.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxk-_HmwXCG5",
        "outputId": "3974db54-7a47-423d-8eb5-2b2c12c57966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxoGIiuaKQJT"
      },
      "outputs": [],
      "source": [
        "class AdversarialBiasMitigation(tf.keras.Model):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Predictor network (X → Y)\n",
        "        self.predictor = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(input_dim, activation='linear')  # Output: Y (continuous)\n",
        "        ])\n",
        "\n",
        "        # Adversary network (Y → Z)\n",
        "        self.adversary = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(1, activation='linear')  # Output: Z (e.g., bias attribute)\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        y_pred = self.predictor(x)\n",
        "        z_pred = self.adversary(y_pred)\n",
        "        return y_pred, z_pred\n",
        "\n",
        "    def debias(self, biased_word_or_embedding):\n",
        "      # Check if input is a string (word) or an array (embedding)\n",
        "      if isinstance(biased_word_or_embedding, str):\n",
        "        embedding = word2vec_model[biased_word_or_embedding]  # Get embedding if word\n",
        "      else:\n",
        "        embedding = biased_word_or_embedding  # Use as is if embedding\n",
        "\n",
        "      # Ensure the input is a TensorFlow tensor\n",
        "      embedding = tf.convert_to_tensor(embedding, dtype=tf.float32)\n",
        "\n",
        "      # Add a batch dimension explicitly\n",
        "      embedding = tf.expand_dims(embedding, axis=0)  # Shape: (1, input_dim)\n",
        "\n",
        "      # Pass through the predictor network to get the debiased embedding\n",
        "      debiased_embedding = self.predictor(embedding).numpy()[0]  # Remove batch dimension\n",
        "\n",
        "      return debiased_embedding\n",
        "\n",
        "\n",
        "\n",
        "def project(grad_W_L1, grad_W_L2):\n",
        "    \"\"\"\n",
        "    Calculates the projection of grad_W_L1 onto grad_W_L2.\n",
        "\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    assert len(grad_W_L1) == len(grad_W_L2), \"Gradient lists must have the same length\"\n",
        "\n",
        "    # Compute dot product\n",
        "    dot_product = tf.add_n([tf.reduce_sum(tf.multiply(g1, g2)) for g1, g2 in zip(grad_W_L1, grad_W_L2)])\n",
        "\n",
        "    # Compute norm of grad_W_L2\n",
        "    norm_W_L2 = tf.add_n([tf.reduce_sum(tf.square(g2)) for g2 in grad_W_L2])\n",
        "\n",
        "    # Avoid division by zero\n",
        "    norm_W_L2 = tf.maximum(norm_W_L2, tf.keras.backend.epsilon())\n",
        "\n",
        "    # Compute scaling factor\n",
        "    scale = dot_product / norm_W_L2\n",
        "\n",
        "    # Compute projection\n",
        "    projection = [scale * g2 for g2 in grad_W_L2]\n",
        "\n",
        "    return projection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training step\n",
        "#@tf.function\n",
        "def train_step(model, x, y, z, optimizer_W, optimizer_U, loss_fn_Y, loss_fn_Z):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Forward pass\n",
        "        y_pred, z_pred = model(x)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_WL1 = loss_fn_Y(y, y_pred)  # Predictor loss (L1)\n",
        "        loss_UL2 = loss_fn_Z(z, z_pred)  # Adversary loss (L2)\n",
        "\n",
        "    # Gradients w.r.t predictor (W) for both losses\n",
        "    grads_WL1 = tape.gradient(loss_WL1, model.predictor.trainable_variables)\n",
        "    grads_WL2 = tape.gradient(loss_UL2, model.predictor.trainable_variables)\n",
        "\n",
        "    # Gradients w.r.t adversary (U)\n",
        "    grads_UL2 = tape.gradient(loss_UL2, model.adversary.trainable_variables)\n",
        "\n",
        "    # Project ∇WL1 onto ∇WL2 and subtract it\n",
        "    proj_WL1_WL2 = project(grads_WL1, grads_WL2)\n",
        "\n",
        "    '''\n",
        "    modified_grads_WL1 = [g1 - p for g1, p in zip(grads_WL1, proj_WL1_WL2)]\n",
        "    modified_grads_WL1 = [g1 - g2 for g1, g2 in zip(modified_grads_WL1, grads_WL2)]'''\n",
        "\n",
        "\n",
        "    modified_grads_WL1 = []\n",
        "\n",
        "    # Subtract both projection and grad_W_L2 from grad_W_L1 while preserving structure\n",
        "    for g1, p, g2 in zip(grads_WL1, proj_WL1_WL2, grads_WL2):\n",
        "        modified_grad = g1 - p - g2\n",
        "        modified_grads_WL1.append(modified_grad)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Apply gradients to predictor (using modified gradients)\n",
        "    optimizer_W.apply_gradients(zip(modified_grads_WL1, model.predictor.trainable_variables))\n",
        "\n",
        "    # Apply gradients to adversary\n",
        "    optimizer_U.apply_gradients(zip(grads_UL2, model.adversary.trainable_variables))\n",
        "\n",
        "    return loss_WL1, loss_UL2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTANTIATE THE MODEL\n",
        "\n",
        "input_dim = 300  # Dimension of word embeddings\n",
        "hidden_dim = 128\n",
        "\n",
        "# Instantiate model\n",
        "model = AdversarialBiasMitigation(input_dim=input_dim)\n",
        "\n",
        "# Optimizers 0.001 0.005 2**-16\n",
        "optimizer_W = tf.keras.optimizers.Adam(learning_rate=2**-16)\n",
        "optimizer_U = tf.keras.optimizers.Adam(learning_rate=2**-16)\n",
        "\n",
        "# Loss functions\n",
        "loss_fn_Y = tf.keras.losses.MeanSquaredError()  # For predictor\n",
        "loss_fn_Z = tf.keras.losses.MeanSquaredError()  # For adversary"
      ],
      "metadata": {
        "id": "nWYG1NB6QlKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute X, Y, and Z\n",
        "X, Y, Z = [], [], []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # Feature: X = -word_a + word_b + word_c (based on your query)\n",
        "    word_a_vec = word2vec_model[row['word_a']]\n",
        "    word_b_vec = word2vec_model[row['word_b']]\n",
        "    word_c_vec = word2vec_model[row['word_c']]\n",
        "    feature_vec = -word_a_vec + word_b_vec + word_c_vec\n",
        "    X.append(feature_vec)\n",
        "\n",
        "    # Label: Y = word_d vector\n",
        "    word_d_vec = word2vec_model[row['word_d']]\n",
        "    Y.append(word_d_vec)\n",
        "\n",
        "    # Bias attribute: Z = scalar projection of word_d onto gender_direction\n",
        "    Z.append(np.dot(word_d_vec, gender_direction))  # Scalar value\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "Z = np.array(Z).reshape(-1, 1)  # Convert to 2D array (batch_size, 1)\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(Z.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeWmdxKKhBbr",
        "outputId": "8c2ae3e4-751f-46c8-c21c-cd9fee907632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19544, 300)\n",
            "(19544, 300)\n",
            "(19544, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(X, Y, Z, batch_size):\n",
        "    \"\"\"Create shuffled batches of data.\"\"\"\n",
        "    data_size = len(X)\n",
        "    indices = np.random.permutation(data_size)\n",
        "    X_shuffled = X[indices]\n",
        "    Y_shuffled = Y[indices]\n",
        "    Z_shuffled = Z[indices]\n",
        "\n",
        "    batches = []\n",
        "    for i in range(0, data_size, batch_size):\n",
        "        X_batch = X_shuffled[i:i+batch_size]\n",
        "        Y_batch = Y_shuffled[i:i+batch_size]\n",
        "        Z_batch = Z_shuffled[i:i+batch_size]\n",
        "        batches.append((X_batch, Y_batch, Z_batch))\n",
        "    return batches"
      ],
      "metadata": {
        "id": "Yt7ZAV1Ah-wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "batches = create_batches(X, Y, Z, batch_size)\n",
        "print(len(batches[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ2hm_YFiC2p",
        "outputId": "9c99d408-6478-4be0-81e6-c19a1ab1e439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(100):  # Number of epochs\n",
        "    total_loss_Y = 0.0\n",
        "    total_loss_Z = 0.0\n",
        "\n",
        "    for X_batch, Y_batch, Z_batch in batches:\n",
        "        # Perform a single training step\n",
        "        batch_loss_Y, batch_loss_Z = train_step(\n",
        "            model,\n",
        "            X_batch,\n",
        "            Y_batch,\n",
        "            Z_batch,\n",
        "            optimizer_W,\n",
        "            optimizer_U,\n",
        "            loss_fn_Y,\n",
        "            loss_fn_Z,\n",
        "        )\n",
        "\n",
        "        total_loss_Y += batch_loss_Y.numpy()\n",
        "        total_loss_Z += batch_loss_Z.numpy()\n",
        "\n",
        "    if epoch==0 or (epoch+1)%10==0:\n",
        "      print(f\"Epoch {epoch+1}: Loss_Y={total_loss_Y / len(batches)}, Loss_Z={total_loss_Z / len(batches)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUryr1KolZiw",
        "outputId": "42335558-12c6-4d80-cef1-aa9d87614e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss_Y=0.009228212521502785, Loss_Z=0.04634236788462385\n",
            "Epoch 10: Loss_Y=0.009193668068916189, Loss_Z=0.04633211679878383\n",
            "Epoch 20: Loss_Y=0.00920256596131652, Loss_Z=0.04633671402005978\n",
            "Epoch 30: Loss_Y=0.009211303620580948, Loss_Z=0.04635128578828538\n",
            "Epoch 40: Loss_Y=0.009180933352730243, Loss_Z=0.04632775460862938\n",
            "Epoch 50: Loss_Y=0.009175163535462097, Loss_Z=0.04632600080343633\n",
            "Epoch 60: Loss_Y=0.00921398829486148, Loss_Z=0.046357476745969524\n",
            "Epoch 70: Loss_Y=0.009147297310965512, Loss_Z=0.046314526702356494\n",
            "Epoch 80: Loss_Y=0.009206138176880046, Loss_Z=0.0463563273447791\n",
            "Epoch 90: Loss_Y=0.009177102552105984, Loss_Z=0.04633769608754056\n",
            "Epoch 100: Loss_Y=0.009163358501383877, Loss_Z=0.04632705370926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model.similar_by_word(\"pilot\",topn=5))\n",
        "debiased=model.debias(\"pilot\")\n",
        "#print(debiased)\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"penis\",topn=5))\n",
        "debiased=model.debias(\"penis\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"wife\",topn=5))\n",
        "debiased=model.debias(\"wife\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"macho\",topn=5))\n",
        "debiased=model.debias(\"macho\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"muscular\",topn=5))\n",
        "debiased=model.debias(\"muscular\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"strongman\",topn=5))\n",
        "debiased=model.debias(\"strongman\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similarity(\"midwife\",\"woman\"))\n",
        "print(word2vec_model.most_similar_cosmul(positive=[model.debias(\"midwife\")],negative=[model.debias(\"woman\")])) # Changed line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHz1sRAKlodv",
        "outputId": "9f8aa8cf-19fa-42e9-9156-607b1f36d117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pilot', 0.6522234678268433), ('piloting', 0.6444098353385925), ('pilots', 0.6399162411689758), ('relaxed_el_Amruni', 0.6388845443725586), ('Samoshin', 0.576824426651001)]\n",
            "[('pilot', 0.5282049775123596), ('relaxed_el_Amruni', 0.48049792647361755), ('eject_safely', 0.47894588112831116), ('HH_1N_Huey', 0.4657941162586212), ('aborts_landing', 0.45653775334358215)]\n",
            "[('penises', 0.7238615155220032), ('genitals', 0.7193513512611389), ('vagina', 0.6902350783348083), ('testicles', 0.689781665802002), ('penile', 0.6632111668586731)]\n",
            "[('penis', 0.5951092839241028), ('penises', 0.5219300389289856), ('erect_penis', 0.5028186440467834), ('genitalia', 0.4980599880218506), ('genitals', 0.4946903586387634)]\n",
            "\n",
            "[('husband', 0.8294167518615723), ('daughter', 0.7662219405174255), ('fiancée', 0.7583051919937134), ('mother', 0.7550682425498962), ('fiancee', 0.7449482679367065)]\n",
            "[('wife', 0.7480432391166687), ('husband', 0.647176206111908), ('daughter', 0.6396773457527161), ('son', 0.6233857870101929), ('daughters', 0.6203823685646057)]\n",
            "\n",
            "[('machismo', 0.7543787360191345), ('manly', 0.724465548992157), ('masculine', 0.6791226267814636), ('hyper_masculine', 0.6728920936584473), ('Macho_macho', 0.6065714359283447)]\n",
            "[('macho', 0.5847579836845398), ('playfight', 0.47934985160827637), ('dry_humped', 0.4739474654197693), ('slack_jawed_yokels', 0.4693150520324707), ('sooooo_cute', 0.45986324548721313)]\n",
            "\n",
            "[('physique', 0.6327391266822815), ('muscularly', 0.6263388395309448), ('muscle', 0.623487651348114), ('Muscular', 0.6145991086959839), ('broad_shouldered', 0.6098844408988953)]\n",
            "[('muscular', 0.5406079292297363), ('scrawny', 0.48299792408943176), ('knock_kneed', 0.47535690665245056), ('pigeon_toed', 0.4666367769241333), ('light_complected', 0.4565909504890442)]\n",
            "\n",
            "[('dictator', 0.6628287434577942), ('strongmen', 0.6029625535011292), ('ruler', 0.5557242035865784), ('autocrat', 0.55482017993927), ('despot', 0.5542343854904175)]\n",
            "[('strongman', 0.5868021249771118), ('Alexander_Lukashenka', 0.4822086989879608), ('President_Askar_Akaev', 0.48115167021751404), ('Askar_Akayev', 0.46787118911743164), ('authoritarian_ruler', 0.46525847911834717)]\n",
            "\n",
            "0.38243768\n",
            "[('Kapoeta', 2.1782400608062744), (\"Plumpy'nut\", 2.147218942642212), ('Kilombero', 2.1302640438079834), ('Agricultural_Inputs', 2.1268484592437744), ('maizemeal', 2.1011605262756348), ('Tanga_Region', 2.0705320835113525), ('Manyara_Region', 2.0626277923583984), ('Katito', 2.038954973220825), ('Aweil', 2.0368540287017822), ('micronutrient_powders', 2.0329833030700684)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model weights\n",
        "model.save_weights('/content/drive/MyDrive/MyModels/debias_trained_2weights.weights.h5')"
      ],
      "metadata": {
        "id": "RP_wLeRnxgDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct the architecture\n",
        "loaded_model = AdversarialBiasMitigation(input_dim=300)\n",
        "\n",
        "# Pass dummy data through the model to implicitly build it\n",
        "dummy_input = tf.random.normal([1, 300])  # Batch size = 1, input_dim = 300\n",
        "_ = loaded_model(dummy_input)  # Forward pass to initialize and build the model\n",
        "\n",
        "\n",
        "# Load weights into the new model instance\n",
        "loaded_model.load_weights('/content/drive/MyDrive/MyModels/debias_trained_2weights.weights.h5')\n"
      ],
      "metadata": {
        "id": "1tyZlwqq7zId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model.similar_by_word(\"penis\",topn=5))\n",
        "debiased=loaded_model.debias(\"penis\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"wife\",topn=5))\n",
        "debiased=loaded_model.debias(\"wife\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"macho\",topn=5))\n",
        "debiased=loaded_model.debias(\"macho\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"muscular\",topn=5))\n",
        "debiased=loaded_model.debias(\"muscular\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"manly\",topn=5))\n",
        "debiased=loaded_model.debias(\"manly\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=5))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"leader\",topn=5))\n",
        "debiased=loaded_model.debias(\"leader\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=10))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similar_by_word(\"nurse\",topn=5))\n",
        "debiased=loaded_model.debias(\"nurse\")\n",
        "print(word2vec_model.similar_by_vector(debiased,topn=10))\n",
        "print()\n",
        "\n",
        "print(word2vec_model.similarity(\"midwife\",\"woman\"))\n",
        "print(word2vec_model.most_similar_cosmul(positive=[word2vec_model[\"nurse\"]],negative=[word2vec_model[\"woman\"]])) # Changed line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxAacyiWCCeV",
        "outputId": "15910d7d-cb9e-4351-9e39-dd53cf69a279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('penises', 0.7238615155220032), ('genitals', 0.7193513512611389), ('vagina', 0.6902350783348083), ('testicles', 0.689781665802002), ('penile', 0.6632111668586731)]\n",
            "[('penis', 0.5951092839241028), ('penises', 0.5219300389289856), ('erect_penis', 0.5028186440467834), ('genitalia', 0.4980599880218506), ('genitals', 0.4946903586387634)]\n",
            "\n",
            "[('husband', 0.8294167518615723), ('daughter', 0.7662219405174255), ('fiancée', 0.7583051919937134), ('mother', 0.7550682425498962), ('fiancee', 0.7449482679367065)]\n",
            "[('wife', 0.7480432391166687), ('husband', 0.647176206111908), ('daughter', 0.6396773457527161), ('son', 0.6233857870101929), ('daughters', 0.6203823685646057)]\n",
            "\n",
            "[('machismo', 0.7543787360191345), ('manly', 0.724465548992157), ('masculine', 0.6791226267814636), ('hyper_masculine', 0.6728920936584473), ('Macho_macho', 0.6065714359283447)]\n",
            "[('macho', 0.5847579836845398), ('playfight', 0.47934985160827637), ('dry_humped', 0.4739474654197693), ('slack_jawed_yokels', 0.4693150520324707), ('sooooo_cute', 0.45986324548721313)]\n",
            "\n",
            "[('physique', 0.6327391266822815), ('muscularly', 0.6263388395309448), ('muscle', 0.623487651348114), ('Muscular', 0.6145991086959839), ('broad_shouldered', 0.6098844408988953)]\n",
            "[('muscular', 0.5406079292297363), ('scrawny', 0.48299792408943176), ('knock_kneed', 0.47535690665245056), ('pigeon_toed', 0.4666367769241333), ('light_complected', 0.4565909504890442)]\n",
            "\n",
            "[('masculine', 0.7488994598388672), ('macho', 0.7244656085968018), ('manliness', 0.6433271765708923), ('overtly_masculine', 0.6387232542037964), ('butch', 0.6379002928733826)]\n",
            "[('manly', 0.5774025321006775), ('f_cked', 0.5066647529602051), ('daddy_daddy', 0.5026897192001343), ('overly_affectionate', 0.49952057003974915), ('ab_**_ch', 0.49874594807624817)]\n",
            "\n",
            "[('leadership', 0.5659863948822021), ('Leader', 0.5506060719490051), ('leaders', 0.5479303598403931), ('premier', 0.5060887336730957), ('president', 0.4555639624595642)]\n",
            "[('leader', 0.4347008466720581), ('Chechen_rebel_Doku_Umarov', 0.3988001346588135), ('Muslim_sultanate_annexed', 0.3833128809928894), ('management_Expesite_streamlines', 0.3800593614578247), ('Uzbekistan_Tajikistan_Kyrgyzstan', 0.37049615383148193), ('strongest', 0.3685770034790039), ('ATMORE_Ala._Alabama', 0.36393097043037415), ('El_Gebali', 0.35237374901771545), ('BY_BILLY_COX', 0.3497026264667511), ('Pletnev_founded', 0.3495666980743408)]\n",
            "\n",
            "[('registered_nurse', 0.7907711267471313), ('nurses', 0.7381672859191895), ('nurse_practitioner', 0.6993103623390198), ('midwife', 0.67276531457901), ('respiratory_therapist', 0.662044882774353)]\n",
            "[('nurse', 0.4512668251991272), ('Maj._Gen._Scott_Gration', 0.40053844451904297), ('Marlene_Cassola', 0.3812802731990814), ('Gen._Scott_Gration', 0.3798021674156189), ('Ashley_Benedik_defense', 0.3796437978744507), ('Jennifer_Bier', 0.3740273714065552), ('wondered_Muehlfelt', 0.37243857979774475), ('Ade_Bachtiar_volunteer', 0.3567771315574646), ('secretary_Beth_Mohle', 0.347703754901886), ('Passenger_Jodie_Wickett', 0.3450857102870941)]\n",
            "\n",
            "0.38243768\n",
            "[('José_Luis_Villegas_jvillegas@sacbee.com', 3.111388921737671), ('antimicrobial_stewardship', 2.609710216522217), ('Rotkreuz', 2.5520288944244385), ('SUBSCOL', 2.5001001358032227), ('Order_Entry_CPOE', 2.471104145050049), ('Cadec_Mobius_TTS', 2.470858335494995), ('EMIS', 2.462146282196045), ('eScholar_Uniq_ID', 2.4160144329071045), ('PMBR', 2.415412187576294), ('PMEL', 2.4057037830352783)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(word2vec_model[\"midwife\"].reshape(1,-1),word2vec_model[\"woman\"].reshape(1,-1)))\n",
        "print(cosine_similarity(loaded_model.debias(\"midwife\").reshape(1,-1),word2vec_model[\"woman\"].reshape(1,-1)))\n",
        "print(cosine_similarity(word2vec_model[\"husband\"].reshape(1,-1),word2vec_model[\"man\"].reshape(1,-1)))\n",
        "print(cosine_similarity(loaded_model.debias(\"husband\").reshape(1,-1),word2vec_model[\"man\"].reshape(1,-1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bDl0n9iy4K1",
        "outputId": "bd4120f0-c521-4d2c-92f6-3ccc441d354c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.38243774]]\n",
            "[[0.10887501]]\n",
            "[[0.3449975]]\n",
            "[[0.28097668]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a,b,c,trained_model):\n",
        "  biased=-word2vec_model[a]+word2vec_model[b]+word2vec_model[c]\n",
        "  print(\"Original 4th word completing analogy\")\n",
        "  print(word2vec_model.similar_by_vector(biased,topn=5))\n",
        "  debiased=trained_model.debias(biased)\n",
        "  print(\"Debiased 4th word completing analogy\")\n",
        "  print(word2vec_model.similar_by_vector(debiased,topn=5))\n"
      ],
      "metadata": {
        "id": "OEeC_NDPSMEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('man','woman','dentist',loaded_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19n90hUlS11W",
        "outputId": "1da3593e-94fc-4a98-a445-4ec8796e80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original 4th word completing analogy\n",
            "[('husband', 0.8645042181015015), ('mother', 0.712462842464447), ('wife', 0.7117722034454346), ('daughter', 0.7000213265419006), ('fiancé', 0.6188782453536987)]\n",
            "Debiased 4th word completing analogy\n",
            "[('wife', 0.5400162935256958), ('husband', 0.5280689597129822), ('father', 0.4422704577445984), ('mom', 0.4396694004535675), ('daughter', 0.43689027428627014)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = word2vec_model.most_similar(positive=['woman', 'doctor'], negative=['man'])\n",
        "print(word2vec_model.most_similar(loaded_model.debias(\"gynecologist\")))\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtBRjiCvghSx",
        "outputId": "6f54a184-d922-4d35-d799-bbdd39c3f618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ANo', 0.42786943912506104), ('PICKAWAY', 0.40764257311820984), ('Sudan', 0.3930776119232178), ('PittsburgMo', 0.3920689821243286), ('Journalistically', 0.3900778889656067), ('gynecologist', 0.38513755798339844), ('Sudanese', 0.3810306787490845), ('Israelis', 0.37683916091918945), ('ConfiDent_®', 0.3766477406024933), ('women', 0.3741796612739563)]\n",
            "[('gynecologist', 0.7093892097473145), ('nurse', 0.6477286219596863), ('doctors', 0.6471461653709412), ('physician', 0.64389967918396), ('pediatrician', 0.6249487996101379), ('obstetrician', 0.6072014570236206), ('midwife', 0.5927063822746277), ('dermatologist', 0.5739567279815674), ('pharmacist', 0.5698872804641724), ('oncologist', 0.5691169500350952)]\n"
          ]
        }
      ]
    }
  ]
}